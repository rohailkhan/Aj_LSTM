{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 How to Develop LSTM in Keras",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5mKY8Rt7FOh"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping,TensorBoard\n",
        "\n",
        "from keras_preprocessing.image import ImageDataGenerator \n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D , LSTM  \n",
        "from keras import regularizers, optimizers\n",
        "import pandas as pd\n",
        "import os ,PIL\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2BGJhms8lHu"
      },
      "source": [
        "![LSTM](https://raw.githubusercontent.com/rohailkhan/data/main/lstm.PNG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Wi7CO77hiC"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebkc11HK7iEL"
      },
      "source": [
        "`units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False, time_major=False, unroll=False, **kwargs)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyH6f4B_4rU3"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(LSTM(units=2, activation='tanh', recurrent_activation='sigmoid',unit_forget_bias=True))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhTUg6YhB5F"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(5, input_shape=(2,1)))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fGOGBlQ_qp7"
      },
      "source": [
        "The first hidden layer in the network must define the number of inputs to expect, e.g. the\n",
        "shape of the input layer. Input must be three-dimensional, comprised of samples, time steps,\n",
        "and features in that order.\n",
        "\n",
        "\n",
        "\n",
        "*   **Samples**. These are the rows in your data. One sample may be one sequence.1st item. (usually not specified.The model assumes one or more samples) So only Time steps and Features are mentioned\n",
        "Note: Not to be confused with **batch_input_shape which also has 3 parameters i.e bacth size, Time steps , Features **\n",
        "\n",
        "*   **Time steps**. These are the past observations for a feature, such as lag variables\n",
        "\n",
        "*   **Features**. These are columns in your data\n",
        "\n",
        "\n",
        "Assuming your data is loaded as a NumPy array, you can convert a 1D or 2D dataset to a 3D dataset \n",
        "\n",
        "Imagine we had 2 columns of input data (X) in a NumPy array. We could treat the two columns as two time steps and reshape it as follows:\n",
        "\n",
        "`data = data.reshape((data.shape[0], data.shape[1], 1))`\n",
        "\n",
        " ( Example of reshaping a NumPy array with **1 feature**)\n",
        "\n",
        "If you would like columns in your 2D data to become features with one time step, you can reshape it as follows:\n",
        "\n",
        "`data = data.reshape((data.shape[0], 1, data.shape[1]))`\n",
        "\n",
        "( Example of reshaping a NumPy array with **1 Time Step**)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bhX6xyE3ESl"
      },
      "source": [
        "Today learning:\n",
        "\n",
        "1- input shape of mode:\n",
        "(Samples , Time_steps, Features)  .Note: Samples not mentioned in model\n",
        "\n",
        "2- Time steps : Past observations  \n",
        "\n",
        "model=Sequential(units=2, input_shape=(Time_steps, Features),activation='tanh', recurrent_activation='sigmoid',unit_forget_bias=True)\n",
        "model.add(LSTM\n",
        "3- Epochs has 1 or 2 batches. 1 Epoch is \"One passing through\" of all the samples through training to update weights\n",
        "\n",
        "4- Batches : Usually weights are updated in Batches. Subset of the data that goes through 'One passing through\" the training after which weights are updated \n",
        "\n",
        "3- Theree types of Batches :\n",
        "Bacth size =Total samples of training dataset (so weights only updated after the whole dataset)\n",
        "Batch size =1 : After every sample wts are updated. (Stochastic gradient descent)\n",
        "Batch size =32...After each batch of 32 or 128 etc, the weights get updated\n",
        "\n",
        "\n",
        "4-  LSTM internal state update. Epoch effects this also\n",
        "\n",
        "model.add(LSTM(2, stateful=True, batch_input_shape=(10, 5, 1)))..batch size 5, times steps=5, features=1\n",
        "\n",
        "\n",
        "5-LSTM will not reset internal state at end of each epoch.We reset it after each epoch using model.reset_states() \n",
        "using for loop in fit\n",
        "for i in range(1000):\n",
        "model.fit(X, y, epochs=1, batch_input_shape=(10, 5, 1))\n",
        "model.reset_states()\n",
        "6- Predictions in case of stateful LSTM : same batch size should be used in predictions as used in stateful lstm\n",
        "\n",
        "7- Make Shuffle=False to preserve state across samples\n",
        "\n",
        "for i in range(1000):\n",
        "model.fit(X, y, epochs=1, shuffle=False, batch_input_shape=(10, 5, 1))\n",
        "model.reset_states()\n",
        "8- Points for reset and training\n",
        "    1- If prediction at the end of each sequence..then make batch_size=1 to reset State after each sequence\n",
        "    2- For longer sequences....reset after the batch....make shuffle=False....and also for very long seq, make batch_size=128\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfTm68L7MiQ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(5, input_shape=(2,1)))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FO4e36-4SLI"
      },
      "source": [
        "## LSTM With Single Input Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFiTuZT23D0Z"
      },
      "source": [
        "from numpy import array\n",
        "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbuQ-JRe5yVq"
      },
      "source": [
        "1 -Reshape data to 1 Sample , 10 Times steps and 1 Feature each step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrBpRUi4586q",
        "outputId": "51da478c-98e2-4450-f81f-b421a38f3dbf"
      },
      "source": [
        "data.reshape((1,10,1)).shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 10, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xBzDeJ6Epf"
      },
      "source": [
        "data=data.reshape((1,10,1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fv1yP3D6wrA"
      },
      "source": [
        "2 -Make model with 32 LSTM units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqulhOrA8fyQ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(10, 1))) # note in input shape we dont specify samples..just the time-steps and Features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r839hJ2FEMU"
      },
      "source": [
        "**Multiple Input features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlF7neEoFIPP",
        "outputId": "6fd89be7-c1f3-4d2c-d695-204aca394954"
      },
      "source": [
        "from numpy import array\n",
        "data = array([\n",
        "[0.1, 1.0],\n",
        "[0.2, 0.9],\n",
        "[0.3, 0.8],\n",
        "[0.4, 0.7],\n",
        "[0.5, 0.6],\n",
        "[0.6, 0.5],\n",
        "[0.7, 0.4],\n",
        "[0.8, 0.3],\n",
        "[0.9, 0.2],\n",
        "[1.0, 0.1]])\n",
        "data.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q75Iwl57FM8R"
      },
      "source": [
        "data=data.reshape(1,10,2)\n",
        "model=Sequential()\n",
        "model.add(LSTM(units=32,input_shape=(10,2)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95I4lMQDN6WM"
      },
      "source": [
        "# Take Away\n",
        "\n",
        "\n",
        "**1- input shape of model**:\n",
        "(Samples , Time_steps, Features)  .Note: Usually Samples not mentioned in model ,Time steps : Past observations  \n",
        "\n",
        "```\n",
        "# model=Sequential()\n",
        "model.add(LSTM (units=2, input_shape=(Time_steps, Features),activation='tanh', recurrent_activation='sigmoid',unit_forget_bias=True)\n",
        "```\n",
        "\n",
        "**2- Epoch** is \"One passing through\" of all the samples through training to update weights .Epochs has 1 or 2 batches.\n",
        "\n",
        "**3- Batches** : Usually weights are updated in Batches. Subset of the data that goes through 'One passing through\" the training after which weights are updated \n",
        "\n",
        "**- Theree types of Batches** :\n",
        "Bacth size =Total samples of training dataset (so weights only updated after the whole dataset)\n",
        "Batch size =1 : After every sample wts are updated. (Stochastic gradient descent)\n",
        "Batch size =32...After each batch of 32 or 128 etc, the weights get updated\n",
        "\n",
        "\n",
        "**4-  LSTM internal state update**. Bacth size impose a tension on efficiency of learning , speed of learning and also influence of internal state (or how often the sate should reset) .Therefore batch size shape should be defined for stateful LSTM\n",
        "\n",
        "```\n",
        "# # batch_input_shape=( size,times, features)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# model.add(LSTM(2, stateful=True, batch_input_shape=(10, 5, 1)))..batch size 5, times steps=5, features=1)\n",
        "```\n",
        "\n",
        "\n",
        "**5-LSTM internal-state reset** : LSTM will not reset internal state at end of each epoch.We reset it after each epoch using model.reset_states() \n",
        "using for loop in fit\n",
        "```\n",
        "# for i in range(1000):\n",
        "    model.fit(X, y, epochs=1, batch_input_shape=(10, 5, 1))\n",
        "    model.reset_states()\n",
        "```\n",
        "\n",
        "**6- Bacth size for Predictions in stateful LSTM** : same batch size should be used in predictions as used in stateful lstm\n",
        "\n",
        "**7- Make Shuffle=False** to preserve same state across samples in case of stateful LSTM\n",
        "\n",
        "\n",
        "```\n",
        "# for i in range(1000):\n",
        "model.fit(X, y, epochs=1, shuffle=False, batch_input_shape=(10, 5, 1))\n",
        "model.reset_states()\n",
        "```\n",
        "\n",
        "\n",
        "**8- Points for reset and training**\n",
        "```\n",
        "    1- If prediction at the end of each sequence..then make batch_size=1 to reset State after each sequence\n",
        "    2- For longer sequences....reset after the batch....make shuffle=False....and also for very long seq, make batch_size=128\n",
        "```"
      ]
    }
  ]
}