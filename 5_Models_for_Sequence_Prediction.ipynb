{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 Models for Sequence Prediction",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMv/SAGda4OFklWnjwksXST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohailkhan/JB-lstm/blob/main/5_Models_for_Sequence_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5mKY8Rt7FOh"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D,  LSTM  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2BGJhms8lHu"
      },
      "source": [
        "![LSTM](https://raw.githubusercontent.com/rohailkhan/data/main/lstm.PNG)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me3jVwpH-BBw"
      },
      "source": [
        "# 1 One to One (I/P=one_time_step , O/P= Dense(1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Wi7CO77hiC"
      },
      "source": [
        "model=Sequential()\n",
        "# just use time_slots=1 in input shape\n",
        "model.add(LSTM(units=3 , input_shape=(1,features))) \n",
        "# input_Shape=(samples, time_slots, features) but usually samples is left blank\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNvEYqs-_3mC"
      },
      "source": [
        "# 2 Many to One (I/P=many_time_steps , O/P= Dense(1)\n",
        "almost same as One to one except in input shape, we dont use time_slots=1 but keep it equal to the input time steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxOU0qwpAASI"
      },
      "source": [
        "model=Sequential()\n",
        "# just use time_slots='no of steps' in input shape\n",
        "model.add(LSTM(units=3 , input_shape=(steps,features))) \n",
        "# input_Shape=(samples, time_slots, features) but usually samples is left blank\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no3wa4EtAbCG"
      },
      "source": [
        "# 2 One to Many\n",
        "### e.g getting text sequence from an Image . We use ***TimeDistributed*** wrapper in order to use **same O/P Layer multiple times** for the requied number of output steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zteUZ1iPB8gj"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(...))\n",
        "...\n",
        "model.add(LSTM(...))\n",
        "model.add(TimeDistributed(Dense(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TK-zRmPCuFI"
      },
      "source": [
        "# Many to Many \n",
        "##1- Fixed length I/P : (input_shape=steps , LSTM (Return Seq true) O/P =TimeDsitributed \n",
        "\n",
        "\n",
        "##2- Variable length  : (Just add Encoder-decoder to map the input time-steps to fixed sized representation\n",
        "LSTM layer must be configured\n",
        "to return a value for each input time step rather than a single value at the end of the input\n",
        "sequence (e.g. return sequences=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH61VBQpAtB2"
      },
      "source": [
        "# In a sense, this model combines the capabilities of the many-to-one and one-to-many models.\n",
        "# If the number of input and output time steps are equal, then the LSTM layer must be configured\n",
        "# to return a value for each input time step rather than a single value at the end of the input\n",
        "# sequence (e.g. return sequences=True) and the same Dense layer can be used to produce one\n",
        "# output time step for each of the input time steps via the TimeDistributed layer wrapper\n",
        "model = Sequential()\n",
        "model.add(LSTM(..., input_shape=(steps, ...), return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApaYEMxb7nk3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(..., input_shape=(in_steps, ...)))\n",
        "model.add(RepeatVector(out_steps))\n",
        "model.add(LSTM(..., return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK2O1mzu_RfB"
      },
      "source": [
        "# **Mapping Applications to Models**\n",
        "# 10 Important Prediction Types in 2 Groups\n",
        "\n",
        "## **Group 1- Time Series**\n",
        "1.   **Univariate Time Series Forecasting** : \n",
        "  \n",
        "  I/P= 1 Series with **Many time steps** , O/P prediction= **1 time-step** --->> Many-to-One\n",
        "2.   **Multivariate Time Series Forecasting** : \n",
        "  \n",
        "  I/P= Many Series with **Many time steps** , O/P prediction= **1 time-step** beyond one or more of the input sequences --->> Many-to-One\n",
        "3.   **Multi-step Time Series Forecasting** : \n",
        " \n",
        "  I/P= Many Series with **Many time steps** , O/P prediction= **1 time-step** beyond one or more of the input sequences --->> Many-to-Many\n",
        "4.   **Time Series Classification** : \n",
        " \n",
        "  I/P= 1 or Many Series with **Many time steps** , O/P prediction= 1 Class -->> Many to One\n",
        "\n",
        "  ************************************************************\n",
        "  ************************************************************\n",
        "\n",
        "## **Group 2- Natural Language Processing**\n",
        "\n",
        "1.   **Image Captioning** : \n",
        "  \n",
        "  I/P= 1 Image, O/P prediction= **Caption sentence** --->> One-to-Many\n",
        "2.   **Video Captioning** : \n",
        "  \n",
        "  I/P= Video , O/P prediction= **Caption sentence** --->> Many-to-Many\n",
        "3.   **Sentiment Analysis** : \n",
        "\n",
        "  I/P=Sentence , O/P prediction= **Sentiment** --->> Many-to-Many\n",
        "    \n",
        "4.   **Text Translation** : \n",
        "  I/P= Text , O/P prediction= **Translation** --->> Many-to-Many\n",
        " \n",
        "5.   **Text Summarization** : \n",
        "  I/P= Text , O/P prediction= **Summary** --->> Many-to-Many"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIJvGsn_q6f"
      },
      "source": [
        "# Cardinality from Time Steps (not Features!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D--p6LHs_rAY"
      },
      "source": [
        "A common point of confusion is to conflate the above examples of sequence mapping models\n",
        "with multiple input and output features. A sequence may be comprised of single values, one for\n",
        "each time step.\n",
        "\n",
        "\n",
        "![One to One](https://raw.githubusercontent.com/rohailkhan/data/main/JB-lstm5.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6hWKdfO-1Ha"
      },
      "source": [
        "# **Two Common Misunderstandings**\n",
        "## Time steps as Input Features\n",
        "Lag observations at previous time steps are framed as input features to the model.\n",
        "## Time steps as Output Features\n",
        "Predictions at multiple future time steps are framed as output features to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5IGwfuljt4v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWY1Bgrrjt73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fGOGBlQ_qp7"
      },
      "source": [
        "The first hidden layer in the network must define the number of inputs to expect, e.g. the\n",
        "shape of the input layer. Input must be three-dimensional, comprised of samples, time steps,\n",
        "and features in that order.\n",
        "\n",
        "\n",
        "\n",
        "*   **Samples**. These are the rows in your data. One sample may be one sequence.1st item. (usually not specified.The model assumes one or more samples) So only Time steps and Features are mentioned\n",
        "Note: Not to be confused with **batch_input_shape which also has 3 parameters i.e bacth size, Time steps , Features **\n",
        "\n",
        "*   **Time steps**. These are the past observations for a feature, such as lag variables\n",
        "\n",
        "*   **Features**. These are columns in your data\n",
        "\n",
        "\n",
        "Assuming your data is loaded as a NumPy array, you can convert a 1D or 2D dataset to a 3D dataset \n",
        "\n",
        "Imagine we had 2 columns of input data (X) in a NumPy array. We could treat the two columns as two time steps and reshape it as follows:\n",
        "\n",
        "`data = data.reshape((data.shape[0], data.shape[1], 1))`\n",
        "\n",
        " ( Example of reshaping a NumPy array with **1 feature**)\n",
        "\n",
        "If you would like columns in your 2D data to become features with one time step, you can reshape it as follows:\n",
        "\n",
        "`data = data.reshape((data.shape[0], 1, data.shape[1]))`\n",
        "\n",
        "( Example of reshaping a NumPy array with **1 Time Step**)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfTm68L7MiQ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(5, input_shape=(2,1)))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fv1yP3D6wrA"
      },
      "source": [
        "2 -Make model with 32 LSTM units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqulhOrA8fyQ"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(10, 1))) # note in input shape we dont specify samples..just the time-steps and Features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r839hJ2FEMU"
      },
      "source": [
        "**Multiple Input features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlF7neEoFIPP",
        "outputId": "6fd89be7-c1f3-4d2c-d695-204aca394954"
      },
      "source": [
        "from numpy import array\n",
        "data = array([\n",
        "[0.1, 1.0],\n",
        "[0.2, 0.9],\n",
        "[0.3, 0.8],\n",
        "[0.4, 0.7],\n",
        "[0.5, 0.6],\n",
        "[0.6, 0.5],\n",
        "[0.7, 0.4],\n",
        "[0.8, 0.3],\n",
        "[0.9, 0.2],\n",
        "[1.0, 0.1]])\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q75Iwl57FM8R"
      },
      "source": [
        "data=data.reshape(1,10,2)\n",
        "model=Sequential()\n",
        "model.add(LSTM(units=32,input_shape=(10,2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95I4lMQDN6WM"
      },
      "source": [
        "\n",
        "**1- input shape of model**:\n",
        "(Samples , Time_steps, Features)  .Note: Usually Samples not mentioned in model ,Time steps : Past observations  \n",
        "\n",
        "```\n",
        "# model=Sequential()\n",
        "model.add(LSTM (units=2, input_shape=(Time_steps, Features),activation='tanh', recurrent_activation='sigmoid',unit_forget_bias=True)\n",
        "```\n",
        "\n",
        "**2- Epoch** is \"One passing through\" of all the samples through training to update weights .Epochs has 1 or 2 batches.\n",
        "\n",
        "**3- Batches** : Usually weights are updated in Batches. Subset of the data that goes through 'One passing through\" the training after which weights are updated \n",
        "\n",
        "**- Theree types of Batches** :\n",
        "Bacth size =Total samples of training dataset (so weights only updated after the whole dataset)\n",
        "Batch size =1 : After every sample wts are updated. (Stochastic gradient descent)\n",
        "Batch size =32...After each batch of 32 or 128 etc, the weights get updated\n",
        "\n",
        "\n",
        "**4-  LSTM internal state update**. Bacth size impose a tension on efficiency of learning , speed of learning and also influence of internal state (or how often the sate should reset) .Therefore batch size shape should be defined for stateful LSTM\n",
        "\n",
        "```\n",
        "# # batch_input_shape=( size,times, features)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# model.add(LSTM(2, stateful=True, batch_input_shape=(10, 5, 1)))..batch size 5, times steps=5, features=1)\n",
        "```\n",
        "\n",
        "\n",
        "**5-LSTM internal-state reset** : LSTM will not reset internal state at end of each epoch.We reset it after each epoch using model.reset_states() \n",
        "using for loop in fit\n",
        "```\n",
        "# for i in range(1000):\n",
        "    model.fit(X, y, epochs=1, batch_input_shape=(10, 5, 1))\n",
        "    model.reset_states()\n",
        "```\n",
        "\n",
        "**6- Bacth size for Predictions in stateful LSTM** : same batch size should be used in predictions as used in stateful lstm\n",
        "\n",
        "**7- Make Shuffle=False** to preserve same state across samples in case of stateful LSTM\n",
        "\n",
        "\n",
        "```\n",
        "# for i in range(1000):\n",
        "model.fit(X, y, epochs=1, shuffle=False, batch_input_shape=(10, 5, 1))\n",
        "model.reset_states()\n",
        "```\n",
        "\n",
        "\n",
        "**8- Points for reset and training**\n",
        "```\n",
        "    1- If prediction at the end of each sequence..then make batch_size=1 to reset State after each sequence\n",
        "    2- For longer sequences....reset after the batch....make shuffle=False....and also for very long seq, make batch_size=128\n",
        "```"
      ]
    }
  ]
}